{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Препроцессинг"
      ],
      "metadata": {
        "id": "itt5PUKh4-91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_4NV4HoG5CUp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwitterDataPreprocessor:\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self.df = None\n",
        "        self.final_df = None\n",
        "        self.train_df = None\n",
        "        self.val_df = None\n",
        "        self.test_df = None\n",
        "\n",
        "    def load_data(self, limit=100000):\n",
        "        self.df = pd.read_csv(self.file_path).iloc[:limit].copy()\n",
        "        self.df.columns = self.df.columns.str.strip()\n",
        "\n",
        "        # Выбор нужных столбцов\n",
        "        columns_needed = ['Weekday', 'Hour', 'Day', 'Reach', 'RetweetCount', 'Likes', 'text']\n",
        "        self.df = self.df[columns_needed].copy()\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_text(text):\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "        text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_text(text):\n",
        "        return \" \".join(str(text).lower().split())\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_keywords_per_tweet(text, top_n=5):\n",
        "        words = str(text).split()\n",
        "        common_words = Counter(words).most_common(top_n)\n",
        "        return \" \".join([word for word, _ in common_words])\n",
        "\n",
        "    def preprocess(self):\n",
        "        self.df['cleaned_text'] = self.df['text'].apply(self.clean_text)\n",
        "        self.df['normalized_text'] = self.df['cleaned_text'].apply(self.normalize_text)\n",
        "        self.df['keywords'] = self.df['normalized_text'].apply(self.extract_keywords_per_tweet)\n",
        "        self.df['hashtags'] = self.df['text'].apply(lambda x: \" \".join(re.findall(r\"#\\w+\", str(x))))\n",
        "\n",
        "        self.final_df = self.df[['Weekday', 'Hour', 'Day', 'Reach', 'RetweetCount', 'Likes', 'text',\n",
        "                                 'cleaned_text', 'normalized_text', 'keywords', 'hashtags']]\n",
        "\n",
        "    def split_and_save(self, output_dir=\"/content\"):\n",
        "        train_df, temp_df = train_test_split(self.final_df, test_size=0.3, random_state=42)\n",
        "        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "        self.train_df = train_df\n",
        "        self.val_df = val_df\n",
        "        self.test_df = test_df\n",
        "\n",
        "        # Сохраняем файлы\n",
        "        train_df.to_csv(f\"{output_dir}/train.csv\", index=False)\n",
        "        val_df.to_csv(f\"{output_dir}/val.csv\", index=False)\n",
        "        test_df.to_csv(f\"{output_dir}/test.csv\", index=False)\n",
        "\n",
        "        print(f\"Train size: {len(train_df)}\")\n",
        "        print(f\"Validation size: {len(val_df)}\")\n",
        "        print(f\"Test size: {len(test_df)}\")\n",
        "\n",
        "        return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "QWa0kFdU5Hu7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/Twitterdatainsheets.csv\"\n",
        "processor = TwitterDataPreprocessor(file_path)\n",
        "\n",
        "processor.load_data()\n",
        "processor.preprocess()\n",
        "train_df, val_df, test_df = processor.split_and_save()\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "dQ4Ok5C65Spg",
        "outputId": "467fc8d0-eeac-4747-e35e-38b7c14bfabf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-11f1d2ab7b11>:11: DtypeWarning: Columns (3,4,5,6,12,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  self.df = pd.read_csv(self.file_path).iloc[:limit].copy()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 70000\n",
            "Validation size: 15000\n",
            "Test size: 15000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Weekday Hour Day    Reach  RetweetCount  Likes  \\\n",
              "76513     Monday   14  14    135.0           0.0    0.0   \n",
              "60406   Thursday   18  25  15571.0           0.0    0.0   \n",
              "27322     Friday    5  29    395.0           0.0    0.0   \n",
              "53699     Sunday    2  21     57.0           0.0    0.0   \n",
              "65412  Wednesday   12   2    106.0           0.0    0.0   \n",
              "\n",
              "                                                    text  \\\n",
              "76513  Netflix Backing Could Pump Up Google Cloud Vs....   \n",
              "60406  Incident response on the AWS cloud and the cas...   \n",
              "27322  Aws Marketplace Channel Development Manager Ca...   \n",
              "53699  We are hiring: Senior Software Engineer - AWS ...   \n",
              "65412  New #Cloudstorage in S. Korea: @ZadaraStorage ...   \n",
              "\n",
              "                                            cleaned_text  \\\n",
              "76513  Netflix Backing Could Pump Up Google Cloud Vs ...   \n",
              "60406  Incident response on the AWS cloud and the cas...   \n",
              "27322  Aws Marketplace Channel Development Manager Ca...   \n",
              "53699  We are hiring Senior Software Engineer AWS Cod...   \n",
              "65412  New Cloudstorage in S Korea expands in AWS Seo...   \n",
              "\n",
              "                                         normalized_text  \\\n",
              "76513  netflix backing could pump up google cloud vs ...   \n",
              "60406  incident response on the aws cloud and the cas...   \n",
              "27322  aws marketplace channel development manager ca...   \n",
              "53699  we are hiring senior software engineer aws cod...   \n",
              "65412  new cloudstorage in s korea expands in aws seo...   \n",
              "\n",
              "                                 keywords            hashtags  \n",
              "76513      netflix aws backing could pump                      \n",
              "60406      cloud the incident response on   #Cloud #Computing  \n",
              "27322  channel seattle wa aws marketplace        #Seattle #WA  \n",
              "53699     senior software engineer aws we                #job  \n",
              "65412         in new cloudstorage s korea  #Cloudstorage #AWS  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74413c3f-7c78-4f8f-9dad-e15253086c12\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Weekday</th>\n",
              "      <th>Hour</th>\n",
              "      <th>Day</th>\n",
              "      <th>Reach</th>\n",
              "      <th>RetweetCount</th>\n",
              "      <th>Likes</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>normalized_text</th>\n",
              "      <th>keywords</th>\n",
              "      <th>hashtags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>76513</th>\n",
              "      <td>Monday</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Netflix Backing Could Pump Up Google Cloud Vs....</td>\n",
              "      <td>Netflix Backing Could Pump Up Google Cloud Vs ...</td>\n",
              "      <td>netflix backing could pump up google cloud vs ...</td>\n",
              "      <td>netflix aws backing could pump</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60406</th>\n",
              "      <td>Thursday</td>\n",
              "      <td>18</td>\n",
              "      <td>25</td>\n",
              "      <td>15571.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Incident response on the AWS cloud and the cas...</td>\n",
              "      <td>Incident response on the AWS cloud and the cas...</td>\n",
              "      <td>incident response on the aws cloud and the cas...</td>\n",
              "      <td>cloud the incident response on</td>\n",
              "      <td>#Cloud #Computing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27322</th>\n",
              "      <td>Friday</td>\n",
              "      <td>5</td>\n",
              "      <td>29</td>\n",
              "      <td>395.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Aws Marketplace Channel Development Manager Ca...</td>\n",
              "      <td>Aws Marketplace Channel Development Manager Ca...</td>\n",
              "      <td>aws marketplace channel development manager ca...</td>\n",
              "      <td>channel seattle wa aws marketplace</td>\n",
              "      <td>#Seattle #WA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53699</th>\n",
              "      <td>Sunday</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>57.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>We are hiring: Senior Software Engineer - AWS ...</td>\n",
              "      <td>We are hiring Senior Software Engineer AWS Cod...</td>\n",
              "      <td>we are hiring senior software engineer aws cod...</td>\n",
              "      <td>senior software engineer aws we</td>\n",
              "      <td>#job</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65412</th>\n",
              "      <td>Wednesday</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>106.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>New #Cloudstorage in S. Korea: @ZadaraStorage ...</td>\n",
              "      <td>New Cloudstorage in S Korea expands in AWS Seo...</td>\n",
              "      <td>new cloudstorage in s korea expands in aws seo...</td>\n",
              "      <td>in new cloudstorage s korea</td>\n",
              "      <td>#Cloudstorage #AWS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74413c3f-7c78-4f8f-9dad-e15253086c12')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-74413c3f-7c78-4f8f-9dad-e15253086c12 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-74413c3f-7c78-4f8f-9dad-e15253086c12');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e0ce3f3a-531f-439d-9960-3e685b265e4a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e0ce3f3a-531f-439d-9960-3e685b265e4a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e0ce3f3a-531f-439d-9960-3e685b265e4a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 70000,\n  \"fields\": [\n    {\n      \"column\": \"Weekday\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Monday\",\n          \"Thursday\",\n          \"Tuesday\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hour\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 48,\n        \"samples\": [\n          \"12\",\n          \"2\",\n          23\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Day\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 62,\n        \"samples\": [\n          \"27\",\n          30,\n          \"14\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Reach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 82548.72445158055,\n        \"min\": 0.0,\n        \"max\": 10342452.0,\n        \"num_unique_values\": 10261,\n        \"samples\": [\n          1467.0,\n          47023.0,\n          1315.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RetweetCount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 115.18382743526915,\n        \"min\": 0.0,\n        \"max\": 26127.0,\n        \"num_unique_values\": 443,\n        \"samples\": [\n          214.0,\n          118.0,\n          128.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Likes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.3785017410610165,\n        \"min\": 0.0,\n        \"max\": 107.0,\n        \"num_unique_values\": 77,\n        \"samples\": [\n          21.0,\n          8.0,\n          27.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51205,\n        \"samples\": [\n          \"\\\"Apple said to move part of cloud business from AWS to Google http://www.seriouslymac.com/2016/03/16/21/11/00/apple-said-to-move-part-of-cloud-business-from-aws-to-google/?utm_medium=feed&utm_source=twitter.com&utm_campaign=Feed:%20seriouslymac http://s.erious.ly\\\"\",\n          \"\\\"Retweeted Amazon Web Services (@awscloud):Check out the latest @AWSforMobile blog: Migrating from #Parse Push... http://twitter.com/awscloud/status/694974649185206272/photo/1?utm_source=fb&utm_medium=fb&utm_campaign=tychons&utm_content=695113285809475585\\\"\",\n          \"RT @ManningBooks: Deal of the Day Feb 14: Amazon Web Services in Action ! Use code dotd021416tw at @hellomichibye @andreaswittig #AWS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 43525,\n        \"samples\": [\n          \"Dropbox y la coopetencia con Amazon Web Services\",\n          \"RT EMPRENDER El 94 del trfico de Tor es malicioso Desde la Wikipedia a Amazon Web Services o e\",\n          \"Bulletproof buys CloudHouse for 10X EBITDA ChannelE2E says But AWS Cloud consultant valuation has stipulations\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"normalized_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 43289,\n        \"samples\": [\n          \"rt turns to the cloud to handle ddoslevel traffic during big event launches via\",\n          \"rt drupalcafe keynote about building a scalable drupal environment on aws\",\n          \"fun thought i wonder how many of the big porn sites are hosted in azure versus aws\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keywords\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 37301,\n        \"samples\": [\n          \"ibm just trounced aws and\",\n          \"job opportunity cloud application architect\",\n          \"cloud in rt warriors led\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hashtags\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10955,\n        \"samples\": [\n          \"#BRE #nfc #IoTworld\",\n          \"#AWS #golang\",\n          \"#AWS #fmecloud\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Класс получение метрик из 3х параметров"
      ],
      "metadata": {
        "id": "-fT-9r1RAt0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TweetMetricsScorer:\n",
        "    def __init__(self, max_reach=None, max_retweet=None, max_likes=None):\n",
        "        self.max_reach = max_reach\n",
        "        self.max_retweet = max_retweet\n",
        "        self.max_likes = max_likes\n",
        "\n",
        "    def fit(self, df):\n",
        "        \"\"\"Автоматически определяет максимумы из датафрейма\"\"\"\n",
        "        self.max_reach = self.max_reach or df['Reach'].max()\n",
        "        self.max_retweet = self.max_retweet or df['RetweetCount'].max()\n",
        "        self.max_likes = self.max_likes or df['Likes'].max()\n",
        "\n",
        "    def score_row(self, reach, retweets, likes):\n",
        "        # Логарифмическое преобразование (добавляем 1, чтобы избежать log(0))\n",
        "        reach_score = np.log1p(reach)\n",
        "        retweet_score = np.log1p(retweets)\n",
        "        likes_score = np.log1p(likes)\n",
        "\n",
        "        # Нормализация\n",
        "        max_total = np.log1p(self.max_reach) * 0.4 + np.log1p(self.max_retweet) * 0.3 + np.log1p(self.max_likes) * 0.3\n",
        "        weighted_score = (0.4 * reach_score + 0.3 * retweet_score + 0.3 * likes_score) / max_total\n",
        "\n",
        "        return round(weighted_score * 10, 2)\n",
        "\n",
        "    def add_score_column(self, df):\n",
        "        self.fit(df)  # убедимся, что максимумы установлены\n",
        "        df['score'] = df.apply(\n",
        "            lambda row: self.score_row(row['Reach'], row['RetweetCount'], row['Likes']),\n",
        "            axis=1\n",
        "        )\n",
        "        return df"
      ],
      "metadata": {
        "id": "XwRQEGOXAiNO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = TweetMetricsScorer()\n",
        "scored_train_df = scorer.add_score_column(train_df)\n",
        "scored_val_df = scorer.add_score_column(val_df)\n",
        "scored_test_df = scorer.add_score_column(test_df)\n",
        "\n",
        "# Сохраняем датафреймы в формате CSV\n",
        "scored_train_df.to_csv('scored_train_data.csv', index=False)\n",
        "scored_val_df.to_csv('scored_val_data.csv', index=False)\n",
        "scored_test_df.to_csv('scored_test_data.csv', index=False)\n",
        "\n",
        "# Выводим примеры данных\n",
        "print(scored_train_df[['Reach', 'RetweetCount', 'Likes', 'score']].head())\n",
        "print(scored_val_df[['Reach', 'RetweetCount', 'Likes', 'score']].head())\n",
        "print(scored_test_df[['Reach', 'RetweetCount', 'Likes', 'score']].head())"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EvEkNsqA-TH",
        "outputId": "201a851a-dc82-40fd-c9d5-325f43ffafb1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Reach  RetweetCount  Likes  score\n",
            "76513    135.0           0.0    0.0   1.80\n",
            "60406  15571.0           0.0    0.0   3.54\n",
            "27322    395.0           0.0    0.0   2.19\n",
            "53699     57.0           0.0    0.0   1.49\n",
            "65412    106.0           0.0    0.0   1.71\n",
            "         Reach  RetweetCount  Likes  score\n",
            "3929   15545.0           0.0    0.0   3.54\n",
            "66365     64.0           0.0    0.0   1.53\n",
            "9267    1108.0           0.0    0.0   2.57\n",
            "23724   1278.0           0.0    0.0   2.62\n",
            "77309     79.0           0.0    0.0   1.61\n",
            "          Reach  RetweetCount  Likes  score\n",
            "81410      21.0           0.0    0.0   1.13\n",
            "71182      48.0           0.0    0.0   1.43\n",
            "44102      86.0           1.0    0.0   1.83\n",
            "69451     939.0           0.0    0.0   2.51\n",
            "78508  538527.0          12.0   20.0   6.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тест"
      ],
      "metadata": {
        "id": "WGidqZ7uJMdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "VM7qANwDi29R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, text_embeddings, numeric_features, scores):\n",
        "        self.text_embeddings = text_embeddings\n",
        "        self.numeric_features = numeric_features\n",
        "        self.scores = scores\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.scores)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.text_embeddings[idx],\n",
        "            self.numeric_features[idx],\n",
        "            self.scores[idx]\n",
        "        )\n",
        "\n",
        "\n",
        "class ViralityRegressor(nn.Module):\n",
        "    def __init__(self, embedding_dim=384, numeric_dim=3, hidden_dim=512, dropout_rate=0.3):\n",
        "        \"\"\"\n",
        "        Модель для предсказания метрики \"виральности\" от 0 до 10.\n",
        "\n",
        "        embedding_dim: размерность эмбеддинга текста\n",
        "        numeric_dim: количество числовых признаков (Weekday, Hour, Day)\n",
        "        hidden_dim: размер скрытого слоя\n",
        "        dropout_rate: вероятность dropout\n",
        "        \"\"\"\n",
        "        super(ViralityRegressor, self).__init__()\n",
        "        input_dim = embedding_dim + numeric_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)  # Одна метрика на выходе\n",
        "\n",
        "    def forward(self, text_embedding, numeric_features):\n",
        "        \"\"\"\n",
        "        text_embedding: тензор [batch_size, embedding_dim]\n",
        "        numeric_features: тензор [batch_size, 3]\n",
        "        \"\"\"\n",
        "        x = torch.cat([text_embedding, numeric_features], dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x.squeeze(1)  # [batch_size]\n",
        "\n",
        "\n",
        "class TweetViralityPredictor:\n",
        "    def __init__(self,\n",
        "                 model_name=\"all-MiniLM-L6-v2\",\n",
        "                 embedding_dim=384,\n",
        "                 hidden_dim=512,\n",
        "                 dropout_rate=0.3,\n",
        "                 learning_rate=0.001,\n",
        "                 batch_size=64,\n",
        "                 use_gpu=True):\n",
        "        \"\"\"\n",
        "        Комплексный класс для предсказания виральности твитов.\n",
        "\n",
        "        Параметры:\n",
        "        model_name: Имя модели для SentenceTransformer\n",
        "        embedding_dim: Размерность эмбеддингов\n",
        "        hidden_dim: Размер скрытого слоя в модели\n",
        "        dropout_rate: Вероятность dropout в модели\n",
        "        learning_rate: Скорость обучения\n",
        "        batch_size: Размер батча для обучения\n",
        "        use_gpu: Использовать ли GPU, если доступно\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # Инициализация embedder\n",
        "        self.embedder = SentenceTransformer(model_name)\n",
        "        self.embedder.to(self.device)\n",
        "\n",
        "        # Инициализация модели\n",
        "        self.model = ViralityRegressor(embedding_dim=embedding_dim,\n",
        "                                       numeric_dim=3,\n",
        "                                       hidden_dim=hidden_dim,\n",
        "                                       dropout_rate=dropout_rate)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Инициализация оптимизатора и планировщика скорости обучения\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.5)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        # Словарь для маппинга категориальных признаков\n",
        "        self.weekday_map = None\n",
        "        self.hour_map = None\n",
        "        self.day_map = None\n",
        "\n",
        "    def _prepare_data(self, df, prediction_mode=False):\n",
        "        \"\"\"\n",
        "        Подготовка данных: получение эмбеддингов и числовых признаков\n",
        "\n",
        "        Параметры:\n",
        "        df: DataFrame с данными\n",
        "        prediction_mode: Если True, не требуется колонка 'score'\n",
        "        \"\"\"\n",
        "        # Проверим колонку score только если не в режиме предсказания\n",
        "        if 'score' not in df.columns and not prediction_mode:\n",
        "            print(\"Нет колонки 'score' в DataFrame\")\n",
        "\n",
        "        # Создадим маппинг для категориальных признаков, если еще не создан\n",
        "        if self.weekday_map is None:\n",
        "            self.weekday_map = {day: i for i, day in enumerate(df['Weekday'].unique())}\n",
        "            self.hour_map = {hour: i for i, hour in enumerate(df['Hour'].unique())}\n",
        "            self.day_map = {day: i for i, day in enumerate(df['Day'].unique())}\n",
        "\n",
        "        # Преобразуем категориальные признаки в числовые\n",
        "        weekday_numeric = df['Weekday'].map(self.weekday_map).values\n",
        "        hour_numeric = df['Hour'].map(self.hour_map).values\n",
        "        day_numeric = df['Day'].map(self.day_map).values\n",
        "\n",
        "        # Объединим числовые признаки\n",
        "        numeric_features = np.column_stack([weekday_numeric, hour_numeric, day_numeric])\n",
        "        numeric_features = torch.tensor(numeric_features, dtype=torch.float32)\n",
        "\n",
        "        # Получаем эмбеддинги для текстов\n",
        "        print(\"Получение эмбеддингов...\")\n",
        "        texts = df['normalized_text'].tolist()\n",
        "\n",
        "        # Используем батчи для обработки больших датасетов\n",
        "        embeddings = []\n",
        "        batch_size = 128  # можно настроить в зависимости от доступной памяти\n",
        "        for i in tqdm(range(0, len(texts), batch_size)):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            batch_embeddings = self.embedder.encode(batch_texts, convert_to_tensor=True)\n",
        "            embeddings.append(batch_embeddings.cpu())\n",
        "\n",
        "        text_embeddings = torch.cat(embeddings, dim=0)\n",
        "\n",
        "        # Получаем целевую переменную (score), если не в режиме предсказания\n",
        "        if prediction_mode:\n",
        "            # При предсказании используем нулевые значения как заглушку\n",
        "            scores = torch.zeros(len(df), dtype=torch.float32)\n",
        "        else:\n",
        "            scores = torch.tensor(df['score'].values, dtype=torch.float32)\n",
        "\n",
        "        return text_embeddings, numeric_features, scores\n",
        "\n",
        "    def train(self, train_df, val_df=None, num_epochs=20):\n",
        "        \"\"\"\n",
        "        Обучение модели на подготовленных данных.\n",
        "\n",
        "        Параметры:\n",
        "        train_df: DataFrame с обучающей выборкой\n",
        "        val_df: DataFrame с валидационной выборкой (опционально)\n",
        "        num_epochs: Количество эпох обучения\n",
        "        \"\"\"\n",
        "        print(f\"Подготовка обучающих данных...\")\n",
        "        train_embeddings, train_numeric, train_scores = self._prepare_data(train_df)\n",
        "\n",
        "        train_dataset = TweetDataset(train_embeddings, train_numeric, train_scores)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        if val_df is not None:\n",
        "            print(f\"Подготовка валидационных данных...\")\n",
        "            val_embeddings, val_numeric, val_scores = self._prepare_data(val_df)\n",
        "            val_dataset = TweetDataset(val_embeddings, val_numeric, val_scores)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
        "\n",
        "        # Обучение модели\n",
        "        print(f\"Обучение модели на {len(train_df)} примерах...\")\n",
        "        self.model.to(self.device)\n",
        "        for epoch in range(num_epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "            for batch_idx, (text_embed, numeric_feats, targets) in enumerate(tqdm(train_loader)):\n",
        "                text_embed = text_embed.to(self.device)\n",
        "                numeric_feats = numeric_feats.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(text_embed, numeric_feats)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            self.lr_scheduler.step()\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # Оценка на валидационной выборке\n",
        "            if val_df is not None:\n",
        "                val_metrics = self.evaluate(val_loader)\n",
        "                print(f\"Validation - MAE: {val_metrics['mae']:.4f}, MSE: {val_metrics['mse']:.4f}\")\n",
        "\n",
        "        print(\"Обучение завершено!\")\n",
        "\n",
        "    def evaluate(self, val_loader=None, test_df=None):\n",
        "        \"\"\"\n",
        "        Оценка производительности модели на валидационных или тестовых данных.\n",
        "\n",
        "        Параметры:\n",
        "        val_loader: DataLoader с валидационной выборкой (опционально)\n",
        "        test_df: DataFrame с тестовой выборкой (опционально)\n",
        "\n",
        "        Возвращает:\n",
        "        Словарь с метриками MAE и MSE\n",
        "        \"\"\"\n",
        "        if test_df is not None:\n",
        "            # Подготовим тестовые данные, если они предоставлены в виде DataFrame\n",
        "            test_embeddings, test_numeric, test_scores = self._prepare_data(test_df)\n",
        "            test_dataset = TweetDataset(test_embeddings, test_numeric, test_scores)\n",
        "            val_loader = DataLoader(test_dataset, batch_size=self.batch_size)\n",
        "\n",
        "        if val_loader is None:\n",
        "            return {\"mae\": None, \"mse\": None}\n",
        "\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for text_embed, numeric_feats, targets in val_loader:\n",
        "                text_embed = text_embed.to(self.device)\n",
        "                numeric_feats = numeric_feats.to(self.device)\n",
        "\n",
        "                outputs = self.model(text_embed, numeric_feats)\n",
        "                all_preds.extend(outputs.cpu().numpy())\n",
        "                all_targets.extend(targets.numpy())\n",
        "\n",
        "        mae = mean_absolute_error(all_targets, all_preds)\n",
        "        mse = mean_squared_error(all_targets, all_preds)\n",
        "\n",
        "        return {\"mae\": mae, \"mse\": mse}\n",
        "\n",
        "    def predict(self, tweets_df):\n",
        "        \"\"\"\n",
        "        Предсказание виральности для новых твитов.\n",
        "\n",
        "        Параметры:\n",
        "        tweets_df: DataFrame с твитами для предсказания. Должен содержать колонки 'normalized_text', 'Weekday', 'Hour', 'Day'\n",
        "\n",
        "        Возвращает:\n",
        "        DataFrame с предсказанными значениями score\n",
        "        \"\"\"\n",
        "        # Проверим, что все необходимые колонки присутствуют\n",
        "        required_cols = ['normalized_text', 'Weekday', 'Hour', 'Day']\n",
        "        for col in required_cols:\n",
        "            if col not in tweets_df.columns:\n",
        "                raise ValueError(f\"Колонка {col} отсутствует в датафрейме\")\n",
        "\n",
        "        # Подготовим данные для предсказания с флагом prediction_mode=True\n",
        "        text_embeddings, numeric_features, dummy_scores = self._prepare_data(tweets_df, prediction_mode=True)\n",
        "\n",
        "        # Создадим датасет и DataLoader\n",
        "        pred_dataset = TweetDataset(text_embeddings, numeric_features, dummy_scores)\n",
        "        pred_loader = DataLoader(pred_dataset, batch_size=self.batch_size)\n",
        "\n",
        "        # Предсказание\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for text_embed, numeric_feats, _ in pred_loader:\n",
        "                text_embed = text_embed.to(self.device)\n",
        "                numeric_feats = numeric_feats.to(self.device)\n",
        "\n",
        "                outputs = self.model(text_embed, numeric_feats)\n",
        "                all_preds.extend(outputs.cpu().numpy())\n",
        "\n",
        "        # Добавим предсказания в датафрейм\n",
        "        result_df = tweets_df.copy()\n",
        "        result_df['predicted_score'] = all_preds\n",
        "\n",
        "        return result_df\n",
        "\n",
        "    def save_model(self, path=\"tweet_virality_model.pt\"):\n",
        "        \"\"\"Сохранение модели и маппингов\"\"\"\n",
        "        model_state = {\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'lr_scheduler_state_dict': self.lr_scheduler.state_dict(),\n",
        "            'weekday_map': self.weekday_map,\n",
        "            'hour_map': self.hour_map,\n",
        "            'day_map': self.day_map,\n",
        "            'model_name': self.model_name,\n",
        "            'embedding_dim': self.embedding_dim\n",
        "        }\n",
        "        torch.save(model_state, path)\n",
        "        print(f\"Модель сохранена в {path}\")\n",
        "\n",
        "    def load_model(self, path=\"tweet_virality_model.pt\"):\n",
        "        \"\"\"Загрузка модели и маппингов\"\"\"\n",
        "        model_state = torch.load(path, map_location=self.device)\n",
        "\n",
        "        # Обновим атрибуты\n",
        "        self.model_name = model_state.get('model_name', self.model_name)\n",
        "        self.embedding_dim = model_state.get('embedding_dim', self.embedding_dim)\n",
        "\n",
        "        # Загрузим маппинги\n",
        "        self.weekday_map = model_state['weekday_map']\n",
        "        self.hour_map = model_state['hour_map']\n",
        "        self.day_map = model_state['day_map']\n",
        "\n",
        "        # Загрузим состояние модели и оптимизатора\n",
        "        self.model.load_state_dict(model_state['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(model_state['optimizer_state_dict'])\n",
        "        self.lr_scheduler.load_state_dict(model_state['lr_scheduler_state_dict'])\n",
        "\n",
        "        print(f\"Модель загружена из {path}\")"
      ],
      "metadata": {
        "id": "TkUmX_q9JOG9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение"
      ],
      "metadata": {
        "id": "8qq6TdEHhP1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример использования класса\n",
        "predictor = TweetViralityPredictor(use_gpu=True)\n",
        "\n",
        "# Обучение модели\n",
        "predictor.train(train_df, val_df, num_epochs=10)\n",
        "\n",
        "# Оценка на тестовом наборе\n",
        "test_metrics = predictor.evaluate(test_df=test_df)\n",
        "print(f\"Test MAE: {test_metrics['mae']:.4f}, MSE: {test_metrics['mse']:.4f}\")\n",
        "\n",
        "# Сохранение модели\n",
        "predictor.save_model(\"tweet_virality_model1.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GT6uTBRzJWfl",
        "outputId": "256c6c5b-784d-4e9c-fa06-88937fa56329"
      },
      "execution_count": 66,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Подготовка обучающих данных...\n",
            "Получение эмбеддингов...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 547/547 [15:58<00:00,  1.75s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Подготовка валидационных данных...\n",
            "Получение эмбеддингов...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [03:25<00:00,  1.74s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Обучение модели на 70000 примерах...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1094/1094 [00:07<00:00, 156.06it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.6803\n",
            "Validation - MAE: 0.5764, MSE: 0.6252\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1094/1094 [00:06<00:00, 176.01it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10, Loss: 0.6159\n",
            "Validation - MAE: 0.5738, MSE: 0.6217\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1094/1094 [00:07<00:00, 146.64it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10, Loss: 0.6032\n",
            "Validation - MAE: 0.5726, MSE: 0.6203\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1094/1094 [00:06<00:00, 170.49it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10, Loss: 0.5972\n",
            "Validation - MAE: 0.5668, MSE: 0.6133\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1094/1094 [00:07<00:00, 144.10it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10, Loss: 0.5898\n",
            "Validation - MAE: 0.5690, MSE: 0.6212\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1094/1094 [00:06<00:00, 158.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10, Loss: 0.5847\n",
            "Validation - MAE: 0.5615, MSE: 0.6038\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1094/1094 [00:07<00:00, 144.32it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10, Loss: 0.5790\n",
            "Validation - MAE: 0.5599, MSE: 0.6035\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1094/1094 [00:07<00:00, 137.93it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10, Loss: 0.5737\n",
            "Validation - MAE: 0.5654, MSE: 0.6127\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1094/1094 [00:07<00:00, 153.68it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10, Loss: 0.5705\n",
            "Validation - MAE: 0.5637, MSE: 0.5941\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1094/1094 [00:08<00:00, 130.07it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10, Loss: 0.5640\n",
            "Validation - MAE: 0.5691, MSE: 0.5985\n",
            "Обучение завершено!\n",
            "Получение эмбеддингов...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [03:22<00:00,  1.72s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test MAE: 0.5644, MSE: 0.5902\n",
            "Модель сохранена в tweet_virality_model1.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Предикт"
      ],
      "metadata": {
        "id": "jOGBVIsWhS5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_data = {\n",
        "    \"text\": \"Amazon Web Services Launches Korean Datacenters for Its Cloud Computing Platform - Business ... http://www.businesswire.com/news/home/20160106006789/en/Amazon-Web-Services-Launches-Korean-Datacenters-Cloud #Cloud #Computing\",\n",
        "    \"Weekday\": \"Wednesday\",\n",
        "    \"Hour\": 20,\n",
        "    \"Day\": 6\n",
        "}"
      ],
      "metadata": {
        "id": "W1TnWGPchZVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функции для обработки текста\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def normalize_text(text):\n",
        "    return \" \".join(str(text).lower().split())\n",
        "\n",
        "# Создаем DataFrame\n",
        "day = tweet_data.get('Day', 1)\n",
        "\n",
        "new_tweets_df = pd.DataFrame([{\n",
        "    'text': tweet_data['text'],\n",
        "    'normalized_text': normalize_text(clean_text(tweet_data['text'])),\n",
        "    'Weekday': tweet_data['Weekday'],\n",
        "    'Hour': tweet_data['Hour'],\n",
        "    'Day': day\n",
        "}])\n"
      ],
      "metadata": {
        "id": "92Y57EZIhVuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка модели\n",
        "new_predictor = TweetViralityPredictor(use_gpu=True)\n",
        "new_predictor.load_model(\"tweet_virality_model.pt\")\n",
        "\n",
        "# Предсказание для новых данных\n",
        "new_tweets_df = new_tweets_df\n",
        "results = new_predictor.predict(new_tweets_df)\n",
        "print(results.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tLjcqesPxOH",
        "outputId": "b569e4a2-9ee8-4875-a921-80591c5877d9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Модель загружена из tweet_virality_model.pt\n",
            "Получение эмбеддингов...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 35.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  Amazon Web Services Launches Korean Datacenter...   \n",
            "\n",
            "                                     normalized_text    Weekday  Hour  Day  \\\n",
            "0  amazon web services launches korean datacenter...  Wednesday    20    6   \n",
            "\n",
            "   predicted_score  \n",
            "0          2.44172  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Улучшение Твитов И Рекомендации"
      ],
      "metadata": {
        "id": "8XfQksX0k3MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "from collections import Counter\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "p3MUXBUvteyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Убедитесь, что NLTK ресурсы загружены\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "class TweetEnhancer:\n",
        "    def __init__(self, historical_df, virality_predictor=None, high_threshold=7.0):\n",
        "        \"\"\"\n",
        "        Инициализация класса для улучшения твитов и выдачи рекомендаций.\n",
        "\n",
        "        Параметры:\n",
        "        historical_df: DataFrame с историческими твитами для анализа.\n",
        "                      Должен содержать колонки 'normalized_text', 'text', 'score', 'Weekday', 'Hour'\n",
        "        virality_predictor: Объект TweetViralityPredictor для предсказания виральности\n",
        "        high_threshold: Порог для определения высокоэффективных твитов (score >= high_threshold)\n",
        "        \"\"\"\n",
        "        self.df = historical_df\n",
        "        self.virality_predictor = virality_predictor\n",
        "        self.high_threshold = high_threshold\n",
        "\n",
        "        # Создаем DataFrame с высокоэффективными твитами\n",
        "        self.high_performing_tweets = self.df[self.df['score'] >= high_threshold]\n",
        "\n",
        "        # Извлекаем полезные паттерны\n",
        "        self._extract_patterns()\n",
        "\n",
        "        # Находим оптимальное время для публикаций\n",
        "        self._analyze_optimal_posting_times()\n",
        "\n",
        "        # Извлекаем полезные хэштеги\n",
        "        self._extract_hashtags()\n",
        "\n",
        "        # Создаем TF-IDF векторайзер для сравнения текстов\n",
        "        self._create_tfidf_vectorizer()\n",
        "\n",
        "    def _extract_patterns(self):\n",
        "        \"\"\"Извлечение полезных паттернов из высокоэффективных твитов\"\"\"\n",
        "        # Паттерны для call-to-action (CTA)\n",
        "        cta_patterns = [\n",
        "            r\"(?i)(retweet|rt|share)(\\s+if|\\s+to|\\s+when|\\s+for)\",\n",
        "            r\"(?i)(like|fav|favorite)(\\s+if|\\s+to|\\s+when|\\s+for)\",\n",
        "            r\"(?i)(follow|subscribe)(\\s+if|\\s+to|\\s+when|\\s+for)\",\n",
        "            r\"(?i)(tag|mention)(\\s+someone|\\s+a friend|\\s+who)\",\n",
        "            r\"(?i)(reply|respond|comment)(\\s+with|\\s+if|\\s+to)\",\n",
        "            r\"(?i)(what do you think|your thoughts|agree\\?)\",\n",
        "            r\"(?i)(don't forget to|remember to|be sure to)\",\n",
        "            r\"(?i)(click|tap|swipe|check out)\",\n",
        "            r\"(?i)(join|participate|take part|vote)\",\n",
        "            r\"(?i)(read more|learn more|find out more)\"\n",
        "        ]\n",
        "\n",
        "        # Словарь для хранения найденных CTA и их эффективности\n",
        "        self.cta_effectiveness = {}\n",
        "\n",
        "        for pattern in cta_patterns:\n",
        "            # Находим твиты с данным паттерном\n",
        "            mask = self.df['text'].str.contains(pattern, regex=True, na=False)\n",
        "            tweets_with_pattern = self.df[mask]\n",
        "\n",
        "            if len(tweets_with_pattern) > 0:\n",
        "                # Среднее значение score для твитов с этим паттерном\n",
        "                avg_score = tweets_with_pattern['score'].mean()\n",
        "                # Количество высокоэффективных твитов с этим паттерном\n",
        "                high_perf_count = len(tweets_with_pattern[tweets_with_pattern['score'] >= self.high_threshold])\n",
        "\n",
        "                # Пример твита с данным паттерном и высоким score\n",
        "                examples = []\n",
        "                if high_perf_count > 0:\n",
        "                    high_perf_examples = tweets_with_pattern[tweets_with_pattern['score'] >= self.high_threshold]\n",
        "                    for _, row in high_perf_examples.head(2).iterrows():\n",
        "                        # Извлекаем сам CTA из твита\n",
        "                        text = row['text']\n",
        "                        match = re.search(pattern, text, re.IGNORECASE)\n",
        "                        if match:\n",
        "                            cta_text = text[match.start():match.end()]\n",
        "                            examples.append((cta_text, row['score']))\n",
        "\n",
        "                # Сохраняем информацию о паттерне\n",
        "                self.cta_effectiveness[pattern] = {\n",
        "                    'avg_score': avg_score,\n",
        "                    'high_perf_count': high_perf_count,\n",
        "                    'total_count': len(tweets_with_pattern),\n",
        "                    'examples': examples\n",
        "                }\n",
        "\n",
        "        # Извлечение эффективных фраз (n-граммы)\n",
        "        self.effective_phrases = self._extract_ngrams()\n",
        "\n",
        "    def _extract_ngrams(self, max_n=3, min_count=3):\n",
        "        \"\"\"\n",
        "        Извлечение эффективных n-грамм из высокоэффективных твитов\n",
        "\n",
        "        Параметры:\n",
        "        max_n: Максимальная длина n-граммы\n",
        "        min_count: Минимальное количество вхождений для учета n-граммы\n",
        "        \"\"\"\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        effective_ngrams = {}\n",
        "\n",
        "        # Токенизация твитов\n",
        "        high_perf_tokens = []\n",
        "        for text in self.high_performing_tweets['normalized_text']:\n",
        "            tokens = [t.lower() for t in word_tokenize(text) if t.isalpha() and t.lower() not in stop_words]\n",
        "            high_perf_tokens.append(tokens)\n",
        "\n",
        "        # Извлечение n-грамм для разных n\n",
        "        for n in range(1, max_n + 1):\n",
        "            ngram_counter = Counter()\n",
        "\n",
        "            for tokens in high_perf_tokens:\n",
        "                token_ngrams = list(ngrams(tokens, n))\n",
        "                ngram_counter.update(token_ngrams)\n",
        "\n",
        "            # Отбор часто встречающихся n-грамм\n",
        "            effective_ngrams[n] = {\n",
        "                ng: count for ng, count in ngram_counter.items()\n",
        "                if count >= min_count\n",
        "            }\n",
        "\n",
        "        return effective_ngrams\n",
        "\n",
        "    def _analyze_optimal_posting_times(self):\n",
        "        \"\"\"Анализ оптимального времени для публикации твитов\"\"\"\n",
        "        # Группировка по дню недели и часу с подсчетом средней виральности\n",
        "        time_analysis = self.df.groupby(['Weekday', 'Hour'])['score'].agg(['mean', 'count']).reset_index()\n",
        "\n",
        "        # Отфильтруем только временные слоты с достаточным количеством твитов (хотя бы 5)\n",
        "        time_analysis = time_analysis[time_analysis['count'] >= 5]\n",
        "\n",
        "        # Отсортируем по убыванию среднего score\n",
        "        self.optimal_posting_times = time_analysis.sort_values(by='mean', ascending=False)\n",
        "\n",
        "    def _extract_hashtags(self):\n",
        "        \"\"\"Извлечение эффективных хэштегов из твитов\"\"\"\n",
        "        # Регулярное выражение для извлечения хэштегов\n",
        "        hashtag_pattern = r'#(\\w+)'\n",
        "\n",
        "        # Словарь для хранения статистики по хэштегам\n",
        "        hashtag_stats = {}\n",
        "\n",
        "        # Извлечение хэштегов из всех твитов\n",
        "        for _, row in self.df.iterrows():\n",
        "            text = row['text']\n",
        "            score = row['score']\n",
        "\n",
        "            hashtags = re.findall(hashtag_pattern, text)\n",
        "            for hashtag in hashtags:\n",
        "                hashtag = hashtag.lower()\n",
        "                if hashtag not in hashtag_stats:\n",
        "                    hashtag_stats[hashtag] = {\n",
        "                        'total_count': 0,\n",
        "                        'high_perf_count': 0,\n",
        "                        'total_score': 0\n",
        "                    }\n",
        "\n",
        "                hashtag_stats[hashtag]['total_count'] += 1\n",
        "                hashtag_stats[hashtag]['total_score'] += score\n",
        "\n",
        "                if score >= self.high_threshold:\n",
        "                    hashtag_stats[hashtag]['high_perf_count'] += 1\n",
        "\n",
        "        # Вычисление среднего score для каждого хэштега\n",
        "        for hashtag in hashtag_stats:\n",
        "            if hashtag_stats[hashtag]['total_count'] > 0:\n",
        "                hashtag_stats[hashtag]['avg_score'] = hashtag_stats[hashtag]['total_score'] / hashtag_stats[hashtag]['total_count']\n",
        "            else:\n",
        "                hashtag_stats[hashtag]['avg_score'] = 0\n",
        "\n",
        "        # Сохраняем только хэштеги, встречающиеся хотя бы 3 раза\n",
        "        self.hashtag_stats = {h: stats for h, stats in hashtag_stats.items() if stats['total_count'] >= 3}\n",
        "\n",
        "        # Сортировка хэштегов по среднему score\n",
        "        self.top_hashtags = sorted(\n",
        "            self.hashtag_stats.items(),\n",
        "            key=lambda x: x[1]['avg_score'],\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "    def _create_tfidf_vectorizer(self):\n",
        "        \"\"\"Создание TF-IDF векторайзера для сравнения текстов\"\"\"\n",
        "        # Обучаем векторайзер на всех текстах\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(\n",
        "            max_features=5000,\n",
        "            stop_words='english',\n",
        "            ngram_range=(1, 2)\n",
        "        )\n",
        "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.df['normalized_text'])\n",
        "\n",
        "    def enhance_tweet(self, tweet_text, include_time_recommendation=True):\n",
        "        \"\"\"\n",
        "        Улучшение твита и предоставление рекомендаций.\n",
        "\n",
        "        Параметры:\n",
        "        tweet_text: Текст твита для улучшения\n",
        "        include_time_recommendation: Включать ли рекомендации по времени публикации\n",
        "\n",
        "        Возвращает:\n",
        "        Словарь с рекомендациями\n",
        "        \"\"\"\n",
        "        # Нормализуем текст так же, как это делается для обучения модели\n",
        "        normalized_text = tweet_text.lower()\n",
        "\n",
        "        # Создаем пустой датафрейм для прогнозирования виральности\n",
        "        current_time = datetime.now()\n",
        "        weekday = current_time.strftime('%A')\n",
        "        hour = current_time.hour\n",
        "        day = current_time.day\n",
        "\n",
        "        tweet_df = pd.DataFrame({\n",
        "            'text': [tweet_text],\n",
        "            'normalized_text': [normalized_text],\n",
        "            'Weekday': [weekday],\n",
        "            'Hour': [hour],\n",
        "            'Day': [day]\n",
        "        })\n",
        "\n",
        "        # Создаем рекомендации\n",
        "        recommendations = {\n",
        "            'original_tweet': tweet_text,\n",
        "            'suggestions': {\n",
        "                'phrasing': self._suggest_phrasing(normalized_text),\n",
        "                'hashtags': self._suggest_hashtags(normalized_text),\n",
        "                'cta': self._suggest_cta(normalized_text)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Предсказание виральности исходного твита\n",
        "        if self.virality_predictor:\n",
        "            predicted = self.virality_predictor.predict(tweet_df)\n",
        "            original_score = predicted['predicted_score'].iloc[0]\n",
        "            recommendations['original_score'] = original_score\n",
        "\n",
        "            # Предсказание виральности улучшенного твита\n",
        "            enhanced_tweet = self._create_enhanced_tweet(tweet_text, recommendations)\n",
        "            enhanced_df = pd.DataFrame({\n",
        "                'text': [enhanced_tweet],\n",
        "                'normalized_text': [enhanced_tweet.lower()],\n",
        "                'Weekday': [weekday],\n",
        "                'Hour': [hour],\n",
        "                'Day': [day]\n",
        "            })\n",
        "\n",
        "            enhanced_predicted = self.virality_predictor.predict(enhanced_df)\n",
        "            enhanced_score = enhanced_predicted['predicted_score'].iloc[0]\n",
        "            recommendations['enhanced_score'] = enhanced_score\n",
        "            recommendations['enhanced_tweet'] = enhanced_tweet\n",
        "            recommendations['improvement'] = enhanced_score - original_score\n",
        "\n",
        "        # Добавляем рекомендации по времени публикации, если требуется\n",
        "        if include_time_recommendation:\n",
        "            recommendations['posting_time'] = self._recommend_posting_time()\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def _suggest_phrasing(self, text, num_suggestions=3):\n",
        "        \"\"\"\n",
        "        Предложение улучшений формулировки на основе эффективных фраз.\n",
        "\n",
        "        Параметры:\n",
        "        text: Текст твита\n",
        "        num_suggestions: Количество предлагаемых улучшений\n",
        "\n",
        "        Возвращает:\n",
        "        Список предложений по улучшению формулировки\n",
        "        \"\"\"\n",
        "        suggestions = []\n",
        "\n",
        "        # Поиск похожих эффективных твитов\n",
        "        text_vector = self.tfidf_vectorizer.transform([text])\n",
        "        similarity_scores = cosine_similarity(text_vector, self.tfidf_matrix).flatten()\n",
        "\n",
        "        # Индексы топ-5 наиболее похожих твитов\n",
        "        most_similar_indices = similarity_scores.argsort()[-5:][::-1]\n",
        "        similar_high_perf_tweets = []\n",
        "\n",
        "        for idx in most_similar_indices:\n",
        "            if self.df.iloc[idx]['score'] >= self.high_threshold:\n",
        "                similar_high_perf_tweets.append(self.df.iloc[idx]['text'])\n",
        "\n",
        "        # Если нашли похожие высокоэффективные твиты, добавляем их как примеры\n",
        "        if similar_high_perf_tweets:\n",
        "            suggestions.append({\n",
        "                'type': 'example',\n",
        "                'description': 'Похожие высокоэффективные твиты для вдохновения:',\n",
        "                'examples': similar_high_perf_tweets[:2]\n",
        "            })\n",
        "\n",
        "        # Проверка наличия эффективных фраз в тексте\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        filtered_tokens = [t for t in tokens if t.isalpha() and t.lower() not in stop_words]\n",
        "\n",
        "        # Проверяем, можно ли добавить эффективные биграммы или триграммы\n",
        "        effective_bigrams = list(self.effective_phrases.get(2, {}).keys())\n",
        "        effective_trigrams = list(self.effective_phrases.get(3, {}).keys())\n",
        "\n",
        "        if effective_bigrams:\n",
        "            # Сортируем по частоте\n",
        "            effective_bigrams.sort(key=lambda x: self.effective_phrases[2][x], reverse=True)\n",
        "            suggestions.append({\n",
        "                'type': 'phrase',\n",
        "                'description': 'Попробуйте добавить эти эффективные фразы:',\n",
        "                'phrases': [' '.join(bigram) for bigram in effective_bigrams[:3]]\n",
        "            })\n",
        "\n",
        "        # Если текст короткий, предложим расширить\n",
        "        if len(filtered_tokens) < 10:\n",
        "            suggestions.append({\n",
        "                'type': 'length',\n",
        "                'description': 'Твит слишком короткий. Попробуйте расширить его, добавив больше контекста или деталей.'\n",
        "            })\n",
        "        # Если текст слишком длинный, предложим сократить\n",
        "        elif len(filtered_tokens) > 30:\n",
        "            suggestions.append({\n",
        "                'type': 'length',\n",
        "                'description': 'Твит слишком длинный. Попробуйте сократить его для лучшего восприятия.'\n",
        "            })\n",
        "\n",
        "        return suggestions\n",
        "\n",
        "    def _suggest_hashtags(self, text, max_suggestions=5):\n",
        "        \"\"\"\n",
        "        Предложение хэштегов на основе анализа текста и эффективных хэштегов.\n",
        "\n",
        "        Параметры:\n",
        "        text: Текст твита\n",
        "        max_suggestions: Максимальное количество предлагаемых хэштегов\n",
        "\n",
        "        Возвращает:\n",
        "        Список рекомендуемых хэштегов\n",
        "        \"\"\"\n",
        "        # Извлечение уже используемых хэштегов\n",
        "        existing_hashtags = set(re.findall(r'#(\\w+)', text.lower()))\n",
        "\n",
        "        # Токенизация для поиска ключевых слов\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        keywords = [t for t in tokens if t.isalpha() and t.lower() not in stop_words]\n",
        "\n",
        "        # Поиск тематически связанных хэштегов\n",
        "        related_hashtags = []\n",
        "\n",
        "        for hashtag, stats in self.hashtag_stats.items():\n",
        "            # Пропускаем уже использованные хэштеги\n",
        "            if hashtag in existing_hashtags:\n",
        "                continue\n",
        "\n",
        "            # Проверяем, связан ли хэштег с ключевыми словами в тексте\n",
        "            if hashtag in keywords or any(hashtag in keyword for keyword in keywords):\n",
        "                related_hashtags.append((hashtag, stats['avg_score']))\n",
        "\n",
        "        # Сортируем по среднему score\n",
        "        related_hashtags.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Если мало тематически связанных хэштегов, добавляем топовые хэштеги\n",
        "        if len(related_hashtags) < max_suggestions:\n",
        "            top_hashtags = [\n",
        "                (hashtag, stats['avg_score'])\n",
        "                for hashtag, stats in self.top_hashtags\n",
        "                if hashtag not in existing_hashtags and not any(hashtag == rh[0] for rh in related_hashtags)\n",
        "            ]\n",
        "\n",
        "            # Объединяем списки и снова сортируем\n",
        "            all_hashtags = related_hashtags + top_hashtags\n",
        "            all_hashtags.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Выбираем top max_suggestions хэштегов\n",
        "            suggested_hashtags = [f\"#{h[0]}\" for h in all_hashtags[:max_suggestions]]\n",
        "        else:\n",
        "            suggested_hashtags = [f\"#{h[0]}\" for h in related_hashtags[:max_suggestions]]\n",
        "\n",
        "        return suggested_hashtags\n",
        "\n",
        "    def _suggest_cta(self, text):\n",
        "        \"\"\"\n",
        "        Предложение улучшений призывов к действию (CTA).\n",
        "\n",
        "        Параметры:\n",
        "        text: Текст твита\n",
        "\n",
        "        Возвращает:\n",
        "        Список предложений по улучшению CTA\n",
        "        \"\"\"\n",
        "        # Проверяем, есть ли уже CTA в твите\n",
        "        has_cta = any(re.search(pattern, text, re.IGNORECASE) for pattern in self.cta_effectiveness)\n",
        "\n",
        "        suggestions = []\n",
        "\n",
        "        if not has_cta:\n",
        "            # Сортируем CTA по эффективности\n",
        "            effective_ctas = sorted(\n",
        "                self.cta_effectiveness.items(),\n",
        "                key=lambda x: (x[1]['high_perf_count'] / max(1, x[1]['total_count']), x[1]['avg_score']),\n",
        "                reverse=True\n",
        "            )\n",
        "\n",
        "            # Выбираем топ-3 эффективных CTA с примерами\n",
        "            top_ctas = []\n",
        "            for pattern, stats in effective_ctas[:3]:\n",
        "                if stats['examples']:\n",
        "                    example_text, example_score = stats['examples'][0]\n",
        "                    top_ctas.append({\n",
        "                        'cta_type': self._get_cta_type(pattern),\n",
        "                        'example': example_text,\n",
        "                        'score': example_score\n",
        "                    })\n",
        "\n",
        "            if top_ctas:\n",
        "                suggestions.append({\n",
        "                    'type': 'cta',\n",
        "                    'description': 'Добавьте призыв к действию (CTA) для увеличения вовлеченности:',\n",
        "                    'examples': top_ctas\n",
        "                })\n",
        "        else:\n",
        "            suggestions.append({\n",
        "                'type': 'cta',\n",
        "                'description': 'В твите уже есть призыв к действию, что хорошо для вовлеченности.'\n",
        "            })\n",
        "\n",
        "        return suggestions\n",
        "\n",
        "    def _get_cta_type(self, pattern):\n",
        "        \"\"\"Определение типа CTA по регулярному выражению\"\"\"\n",
        "        if 'retweet' in pattern.lower() or 'rt' in pattern.lower() or 'share' in pattern.lower():\n",
        "            return 'Ретвит/Репост'\n",
        "        elif 'like' in pattern.lower() or 'fav' in pattern.lower():\n",
        "            return 'Лайк'\n",
        "        elif 'follow' in pattern.lower() or 'subscribe' in pattern.lower():\n",
        "            return 'Подписка'\n",
        "        elif 'tag' in pattern.lower() or 'mention' in pattern.lower():\n",
        "            return 'Упоминание'\n",
        "        elif 'reply' in pattern.lower() or 'respond' in pattern.lower() or 'comment' in pattern.lower():\n",
        "            return 'Комментарий'\n",
        "        elif 'think' in pattern.lower() or 'thoughts' in pattern.lower() or 'agree' in pattern.lower():\n",
        "            return 'Вопрос/Опрос'\n",
        "        else:\n",
        "            return 'Общий призыв'\n",
        "\n",
        "    def _recommend_posting_time(self, top_n=3):\n",
        "        \"\"\"\n",
        "        Рекомендация оптимального времени для публикации.\n",
        "\n",
        "        Параметры:\n",
        "        top_n: Количество рекомендуемых временных слотов\n",
        "\n",
        "        Возвращает:\n",
        "        Список рекомендуемых временных слотов с метриками\n",
        "        \"\"\"\n",
        "        # Берем топ-N временных слотов\n",
        "        top_times = self.optimal_posting_times.head(top_n)\n",
        "\n",
        "        recommended_times = []\n",
        "        for _, row in top_times.iterrows():\n",
        "            recommended_times.append({\n",
        "                'weekday': row['Weekday'],\n",
        "                'hour': row['Hour'],\n",
        "                'avg_score': row['mean'],\n",
        "                'sample_size': row['count']\n",
        "            })\n",
        "\n",
        "        return recommended_times\n",
        "\n",
        "    def _create_enhanced_tweet(self, original_tweet, recommendations):\n",
        "        \"\"\"\n",
        "        Создание улучшенной версии твита на основе рекомендаций.\n",
        "\n",
        "        Параметры:\n",
        "        original_tweet: Исходный текст твита\n",
        "        recommendations: Словарь с рекомендациями\n",
        "\n",
        "        Возвращает:\n",
        "        Улучшенный текст твита\n",
        "        \"\"\"\n",
        "        enhanced_tweet = original_tweet\n",
        "\n",
        "        # Удаляем существующие хэштеги для замены на рекомендованные\n",
        "        enhanced_tweet = re.sub(r'#\\w+', '', enhanced_tweet).strip()\n",
        "\n",
        "        # Добавляем рекомендованные хэштеги\n",
        "        if recommendations['suggestions']['hashtags']:\n",
        "            hashtag_text = ' ' + ' '.join(recommendations['suggestions']['hashtags'][:3])\n",
        "            enhanced_tweet += hashtag_text\n",
        "\n",
        "        # Добавляем CTA, если его нет\n",
        "        cta_suggestions = recommendations['suggestions']['cta']\n",
        "        has_cta = any(suggestion['description'].startswith('В твите уже есть призыв') for suggestion in cta_suggestions)\n",
        "\n",
        "        if not has_cta and len(cta_suggestions) > 0 and 'examples' in cta_suggestions[0]:\n",
        "            # Добавляем первый пример CTA\n",
        "            example = cta_suggestions[0]['examples'][0]['example']\n",
        "            if not re.search(re.escape(example), enhanced_tweet, re.IGNORECASE):\n",
        "                enhanced_tweet += f\" {example}\"\n",
        "\n",
        "        return enhanced_tweet.strip()\n",
        "\n",
        "    def get_content_insights(self):\n",
        "        \"\"\"\n",
        "        Получение обобщенных инсайтов о контенте.\n",
        "\n",
        "        Возвращает:\n",
        "        Словарь с инсайтами о контенте\n",
        "        \"\"\"\n",
        "        insights = {\n",
        "            'top_posting_times': self._recommend_posting_time(5),\n",
        "            'top_hashtags': [{'hashtag': h, 'avg_score': stats['avg_score']}\n",
        "                             for h, stats in self.top_hashtags[:10]],\n",
        "            'effective_ctas': [],\n",
        "            'content_patterns': []\n",
        "        }\n",
        "\n",
        "        # Топ-5 эффективных CTA\n",
        "        effective_ctas = sorted(\n",
        "            self.cta_effectiveness.items(),\n",
        "            key=lambda x: (x[1]['high_perf_count'] / max(1, x[1]['total_count']), x[1]['avg_score']),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        for pattern, stats in effective_ctas[:5]:\n",
        "            if stats['examples']:\n",
        "                insights['effective_ctas'].append({\n",
        "                    'cta_type': self._get_cta_type(pattern),\n",
        "                    'example': stats['examples'][0][0] if stats['examples'] else '',\n",
        "                    'avg_score': stats['avg_score'],\n",
        "                    'usage_count': stats['total_count']\n",
        "                })\n",
        "\n",
        "        # Эффективные фразы (униграммы, биграммы)\n",
        "        for n in [1, 2]:\n",
        "            if n in self.effective_phrases:\n",
        "                top_ngrams = sorted(\n",
        "                    self.effective_phrases[n].items(),\n",
        "                    key=lambda x: x[1],\n",
        "                    reverse=True\n",
        "                )[:5]\n",
        "\n",
        "                for ngram, count in top_ngrams:\n",
        "                    insights['content_patterns'].append({\n",
        "                        'phrase': ' '.join(ngram),\n",
        "                        'count': count\n",
        "                    })\n",
        "\n",
        "        return insights"
      ],
      "metadata": {
        "id": "caWLXK3mk58W"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePpQ-uJonIOb",
        "outputId": "8713c096-fb7f-48f1-e432-fc55a2079d51"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedTweetEnhancer(TweetEnhancer):\n",
        "    def __init__(self, historical_df, virality_predictor=None, high_threshold=7.0):\n",
        "        super().__init__(historical_df, virality_predictor, high_threshold)\n",
        "        # Популярные эмодзи и их категории\n",
        "        self.emoji_dict = {\n",
        "            'positive': ['👍', '😊', '🎉', '✨', '🔥', '💯', '🙌', '❤️', '🚀', '💪'],\n",
        "            'tech': ['💻', '📱', '🖥️', '☁️', '📊', '🔧', '🌐', '📈', '🤖', '📡'],\n",
        "            'business': ['💼', '📂', '📈', '💰', '🏢', '🤝', '📊', '📝', '💸', '🔍'],\n",
        "            'announcement': ['📢', '🔔', '📣', '✅', '⚠️', '📌', '🆕', '🔖', '📰', '🎯']\n",
        "        }\n",
        "\n",
        "    def _suggest_emojis(self, text, max_emojis=2):\n",
        "        \"\"\"Предлагает эмодзи релевантные тексту твита\"\"\"\n",
        "        # Простая логика для определения подходящих эмодзи\n",
        "        text_lower = text.lower()\n",
        "        suggested_emojis = []\n",
        "\n",
        "        # Анализ текста для выбора категории эмодзи\n",
        "        if any(word in text_lower for word in ['launch', 'new', 'announce', 'introducing']):\n",
        "            suggested_emojis.extend(self.emoji_dict['announcement'][:max_emojis])\n",
        "\n",
        "        if any(word in text_lower for word in ['tech', 'technology', 'computing', 'cloud', 'data', 'digital']):\n",
        "            suggested_emojis.extend(self.emoji_dict['tech'][:max_emojis])\n",
        "\n",
        "        if any(word in text_lower for word in ['business', 'company', 'enterprise', 'corporate']):\n",
        "            suggested_emojis.extend(self.emoji_dict['business'][:max_emojis])\n",
        "\n",
        "        # Добавляем некоторые позитивные эмодзи по умолчанию, если не выбрали других\n",
        "        if len(suggested_emojis) < max_emojis:\n",
        "            suggested_emojis.extend(self.emoji_dict['positive'][:max_emojis - len(suggested_emojis)])\n",
        "\n",
        "        return suggested_emojis[:max_emojis]\n",
        "\n",
        "    def enhance_tweet(self, tweet_text, include_time_recommendation=True):\n",
        "        \"\"\"Расширенная версия метода с поддержкой эмодзи\"\"\"\n",
        "        # Получаем базовые рекомендации от родительского класса\n",
        "        recommendations = super().enhance_tweet(tweet_text, include_time_recommendation)\n",
        "\n",
        "        # Добавляем рекомендации по эмодзи\n",
        "        recommendations['suggestions']['emojis'] = self._suggest_emojis(tweet_text)\n",
        "\n",
        "        # Применяем эмодзи к улучшенному твиту\n",
        "        if 'enhanced_tweet' in recommendations:\n",
        "            emoji_str = ' ' + ' '.join(recommendations['suggestions']['emojis'])\n",
        "            recommendations['enhanced_tweet'] += emoji_str\n",
        "\n",
        "            # Если есть предиктор, обновляем оценку с эмодзи\n",
        "            if self.virality_predictor:\n",
        "                current_time = datetime.now()\n",
        "                weekday = current_time.strftime('%A')\n",
        "                hour = current_time.hour\n",
        "                day = current_time.day\n",
        "\n",
        "                emoji_df = pd.DataFrame({\n",
        "                    'text': [recommendations['enhanced_tweet']],\n",
        "                    'normalized_text': [recommendations['enhanced_tweet'].lower()],\n",
        "                    'Weekday': [weekday],\n",
        "                    'Hour': [hour],\n",
        "                    'Day': [day]\n",
        "                })\n",
        "\n",
        "                enhanced_predicted = self.virality_predictor.predict(emoji_df)\n",
        "                recommendations['enhanced_score_with_emoji'] = enhanced_predicted['predicted_score'].iloc[0]\n",
        "                recommendations['improvement_with_emoji'] = recommendations['enhanced_score_with_emoji'] - recommendations['original_score']\n",
        "\n",
        "        return recommendations"
      ],
      "metadata": {
        "id": "Dwn5YJO7r5SX"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка исторических данных\n",
        "train_df['normalized_text'] = train_df['normalized_text'].fillna(\"\")\n",
        "train_df['text'] = train_df['text'].fillna(\"\")\n",
        "historical_df = train_df\n",
        "\n",
        "# Загрузка модели предсказания виральности\n",
        "predictor = TweetViralityPredictor(use_gpu=True)\n",
        "predictor.load_model(\"tweet_virality_model.pt\")\n",
        "\n",
        "enhancer = EnhancedTweetEnhancer(historical_df, virality_predictor=predictor)\n",
        "\n",
        "# Улучшение конкретного твита\n",
        "tweet = \"Current status: Setting up linux server on Amazon Web Services. Next week we'll be negotiating design and rules.\"\n",
        "recommendations = enhancer.enhance_tweet(tweet)\n",
        "\n",
        "# Вывод рекомендаций\n",
        "print(f\"\\n\\nИсходный твит: {recommendations['original_tweet']}\")\n",
        "print(f\"Прогнозируемая виральность: {recommendations['original_score']:.2f}/10\")\n",
        "print(f\"\\nУлучшенный твит: {recommendations['enhanced_tweet']}\")\n",
        "print(f\"Прогнозируемая виральность улучшенного твита: {recommendations['enhanced_score']:.2f}/10\")\n",
        "print(f\"\\nУлучшенный твит с эмодзи: {recommendations['enhanced_tweet']}\")\n",
        "print(f\"Виральность с эмодзи: {recommendations.get('enhanced_score_with_emoji', 0):.2f}/10\")\n",
        "print(f\"Улучшение с эмодзи: {recommendations.get('improvement_with_emoji', 0):.2f} пунктов\")\n",
        "\n",
        "# Выводим рекомендации по времени публикации\n",
        "if 'posting_time' in recommendations:\n",
        "    print(\"\\nРекомендуемое время публикации:\")\n",
        "    for time_slot in recommendations['posting_time']:\n",
        "        print(f\"- {time_slot['weekday']}, {time_slot['hour']}:00, Средний рейтинг: {time_slot['avg_score']:.2f}\")\n",
        "\n",
        "# Получение общих инсайтов\n",
        "insights = enhancer.get_content_insights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyhDXZDHlF_g",
        "outputId": "c40e8123-bed8-4e95-c55a-b7d8bf57c859"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Модель загружена из tweet_virality_model.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-b41280054f0d>:60: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  mask = self.df['text'].str.contains(pattern, regex=True, na=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Получение эмбеддингов...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 29.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Получение эмбеддингов...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 32.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Получение эмбеддингов...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 31.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Исходный твит: Current status: Setting up linux server on Amazon Web Services. Next week we'll be negotiating design and rules.\n",
            "Прогнозируемая виральность: 2.34/10\n",
            "\n",
            "Улучшенный твит: Current status: Setting up linux server on Amazon Web Services. Next week we'll be negotiating design and rules. #vic #w #e Learn more 👍 😊\n",
            "Прогнозируемая виральность улучшенного твита: 2.40/10\n",
            "\n",
            "Улучшенный твит с эмодзи: Current status: Setting up linux server on Amazon Web Services. Next week we'll be negotiating design and rules. #vic #w #e Learn more 👍 😊\n",
            "Виральность с эмодзи: 2.38/10\n",
            "Улучшение с эмодзи: 0.03 пунктов\n",
            "\n",
            "Рекомендуемое время публикации:\n",
            "- Saturday, 13:00, Средний рейтинг: 2.82\n",
            "- Thursday, 16:00, Средний рейтинг: 2.74\n",
            "- Sunday, 4:00, Средний рейтинг: 2.70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EujoA5d36aKF",
        "qKM5fN-s-Ot1"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}